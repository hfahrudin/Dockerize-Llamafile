# .env

# The URL where the model file can be accessed
MODEL_URL=https://huggingface.co/Mozilla/Llama-3.2-3B-Instruct-llamafile/resolve/main/Llama-3.2-3B-Instruct.Q6_K.llamafile

# The name of the model file
MODEL_NAME=Llama-3.2-3B-Instruct.Q6_K.llamafile

# Optional variables (defaults as needed)
MODEL_PORT=8080                # Port for the model service
MODEL_THREADS=4                # Number of threads for the model
MODEL_BATCH=4                  # Batch size for the model
MODEL_CONTEXT=36000            # Context size for the model
MODEL_NUM_PROCESSORS=4        # Number of processors to use